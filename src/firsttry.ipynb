{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing, Averaging over all csv-files and Saving the averaged data into new csv-file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T08:00:49.688135Z",
     "start_time": "2024-08-16T08:00:42.516965Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json \n",
    "from collections import Counter\n",
    "\n",
    "# extract the code into a function called preprocess_data so that we can use it later for new data\n",
    "def preprocess_data(averaged_df, directory, file):\n",
    "    df = pd.read_csv(directory + file, sep=';')\n",
    "    \n",
    "    # Drop unnecessary and outdated columns\n",
    "    deleted_columns = ['timeStampNetwork', 'timeStampGPS', 'locationDescription', 'people', 'latitudeGPS', 'longitudeGPS', 'latitudeNetwork', 'longitudeNetwork', 'minCn0GPS', 'maxCn0GPS', 'meanCn0GPS', 'minCn0Bluetooth', 'maxCn0Bluetooth', 'minCn0Wifi', 'maxCn0Wifi', 'meanCn0Wifi', 'bAccuracyNetwork', 'speedAccuracyNetwork', 'cellType', 'networkLocationType', 'hAccuracyNetwork', 'vAccuracyNetwork', 'speedAccuracyNetwork', 'bAccuracyNetwork', 'nrBlDevices', 'hAccuracyGPS', 'minCn0Bl', 'meanCn0Bl','maxCn0Bl', 'bAccuracyGPS', 'speedAccuracyGPS', 'vAccuracyGPS', 'nrWifiDevices']\n",
    "    df = safe_delete(df, deleted_columns)\n",
    "    \n",
    "    # Remove first x rows and reset begin index to 0\n",
    "    removedRows = 3\n",
    "    df = df.iloc[removedRows:]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Load satellites json\n",
    "    df['satellites'] = df['satellites'].apply(lambda x: json.loads(x))\n",
    "    \n",
    "    # Add cn0 column for easier computation of statistics\n",
    "    df['satellite_cn0'] = df['satellites'].apply(lambda x: [sat['cn0'] for sat in x])\n",
    "    \n",
    "    # Calculate min, max, mean. median, mode, variance, standard deviation and range of the satellite cn0\n",
    "    df['satellite_cn0_min'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).min() if not pd.Series(x).empty else 0)\n",
    "    df['satellite_cn0_max'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).max() if not pd.Series(x).empty else 0)\n",
    "    df['satellite_cn0_mean'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).mean() if not pd.Series(x).empty else 0)    \n",
    "    df['satellite_cn0_median'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).median() if not pd.Series(x).empty else 0)\n",
    "    df['satellite_cn0_mode'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).mode()[0] if not pd.Series(x).mode().empty else 0)\n",
    "    df['satellite_cn0_std'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).std() if not pd.Series(x).empty else 0)\n",
    "    df['satellite_cn0_range'] = df['satellite_cn0'].apply(lambda x: pd.Series(x).max() - pd.Series(x).min() if not pd.Series(x).empty else 0)\n",
    "    \n",
    "    # load the bluetooth json and load rssi into a new column\n",
    "    df['bluetoothDevices'] = df['bluetoothDevices'].apply(lambda x: json.loads(x))\n",
    "    df['bluetooth_rssi'] = df['bluetoothDevices'].apply(lambda x: [device['rssi'] for device in x])\n",
    "    \n",
    "     # Calculate statistical figures for the bluetooth devices\n",
    "    df['bluetooth_rssi_min'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).min() if not pd.Series(x).empty else 0)\n",
    "    df['bluetooth_rssi_max'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).max() if not pd.Series(x).empty else 0)\n",
    "    df['bluetooth_rssi_mean'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).mean() if not pd.Series(x).empty else 0)\n",
    "    df['bluetooth_rssi_median'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).median() if not pd.Series(x).empty else 0)\n",
    "    df['bluetooth_rssi_mode'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).mode()[0] if not pd.Series(x).mode().empty else 0)\n",
    "    df['bluetooth_rssi_std'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).std() if not pd.Series(x).empty else 0)\n",
    "    df['bluetooth_rssi_range'] = df['bluetooth_rssi'].apply(lambda x: pd.Series(x).max() - pd.Series(x).min() if not pd.Series(x).empty else 0)\n",
    "    \n",
    "    # load the wifi json and load rssi into a new column\n",
    "    df['wifiDevices'] = df['wifiDevices'].apply(lambda x: json.loads(x))\n",
    "    df['wifi_rssi'] = df['wifiDevices'].apply(lambda x: [device['level'] for device in x])\n",
    "    \n",
    "     # Calculate statistical figures for the wifi devices\n",
    "    df['wifi_rssi_min'] = df['wifi_rssi'].apply(lambda x: pd.Series(x).min() if not pd.Series(x).empty else 0)\n",
    "    df['wifi_rssi_max'] = df['wifi_rssi'].apply(lambda x: pd.Series(x).max() if not pd.Series(x).empty else 0)\n",
    "    df['wifi_rssi_mean'] = df['wifi_rssi'].apply(lambda x: pd.Series(x).mean() if not pd.Series(x).empty else 0)\n",
    "    df['wifi_rssi_median'] = df['wifi_rssi'].apply(lambda x: pd.Series(x).median() if not pd.Series(x).empty else 0)\n",
    "    df['wifi_rssi_mode'] = df['wifi_rssi'].apply(lambda x: pd.Series(x).mode()[0] if not pd.Series(x).mode().empty else 0)\n",
    "    df['wifi_rssi_range'] = df['wifi_rssi'].apply(lambda x: pd.Series(x).max() - pd.Series(x).min() if not pd.Series(x).empty else 0)\n",
    "    \n",
    "    # Drop list columns\n",
    "    df.drop(columns=['satellites', 'bluetoothDevices', 'wifiDevices', 'satellite_cn0', 'bluetooth_rssi', 'wifi_rssi'], inplace=True)\n",
    "    \n",
    "     # Average over all columns for numeric values and take the first of non-numeric to have a single row\n",
    "    df_label = df.iloc[0]['label']\n",
    "    \n",
    "    df.drop(columns=['label'], inplace=True)\n",
    "    \n",
    "    df = df.mean().to_frame().T\n",
    "    \n",
    "    df['label'] = df_label\n",
    "    \n",
    "    #place label at the beginning\n",
    "    cols = list(df.columns)\n",
    "    cols = [cols[-1]] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    \n",
    "    averaged_df = pd.concat([averaged_df, df], ignore_index=True)\n",
    "    return averaged_df\n",
    "\n",
    "def safe_delete(df, columns):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(columns=[column])\n",
    "    return df\n",
    "\n",
    "files_directory = '../Daten/firsttry/'\n",
    "averaged_path = '../Daten/averaged_data.csv'\n",
    "\n",
    "if os.path.exists(averaged_path):\n",
    "    os.remove(averaged_path)\n",
    "    print(f\"Removed {averaged_path}\")\n",
    "\n",
    "# Get list of CSV files\n",
    "csv_files = [file for file in os.listdir(files_directory) if file.endswith('.csv')]\n",
    "print(f'Found {len(csv_files)} CSV files')\n",
    "\n",
    "# Get nr of files, where name starts with indoor and outdoor\n",
    "indoor_files = [file for file in csv_files if file.startswith('Indoor')]\n",
    "outdoor_files = [file for file in csv_files if file.startswith('Outdoor')]\n",
    "print(f'Found {len(indoor_files)} indoor files and {len(outdoor_files)} outdoor files')\n",
    "\n",
    "averaged_data = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    averaged_data = preprocess_data(averaged_data, files_directory, file)\n",
    "\n",
    "averaged_data.sort_values(by=['label'], inplace=True)\n",
    "print(averaged_data.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 189 CSV files\n",
      "Found 105 indoor files and 84 outdoor files\n",
      "Index(['label', 'cellStrength', 'nrSatellitesInView', 'nrSatellitesInFix',\n",
      "       'satellite_cn0_min', 'satellite_cn0_max', 'satellite_cn0_mean',\n",
      "       'satellite_cn0_median', 'satellite_cn0_mode', 'satellite_cn0_std',\n",
      "       'satellite_cn0_range', 'bluetooth_rssi_min', 'bluetooth_rssi_max',\n",
      "       'bluetooth_rssi_mean', 'bluetooth_rssi_median', 'bluetooth_rssi_mode',\n",
      "       'bluetooth_rssi_std', 'bluetooth_rssi_range', 'wifi_rssi_min',\n",
      "       'wifi_rssi_max', 'wifi_rssi_mean', 'wifi_rssi_median', 'wifi_rssi_mode',\n",
      "       'wifi_rssi_range'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Randomize and Split the Data for Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomize the data\n",
    "averaged_data = averaged_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# drop the label column\n",
    "X = averaged_data.drop(columns=['label'], axis=1)\n",
    "Y = averaged_data['label']\n",
    "\n",
    "# Split the data into training and testing data\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.0001)\n",
    "X_train = X\n",
    "Y_train = Y\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "#print(f'Testing data shape: {X_test.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T08:00:49.751667Z",
     "start_time": "2024-08-16T08:00:49.689873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (189, 23)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Random Forest Classifier with regularization\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "#Load the model (to retrain it comment the next line and uncomment the line above)\n",
    "clf = joblib.load('../Daten/random_forest_classifier.joblib')\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(clf, X_train, Y_train, cv=10)\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "\n",
    "# Predict the labels of the test data\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "#accuracy = accuracy_score(Y_test, y_pred)\n",
    "#print(f'Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T08:00:50.827311Z",
     "start_time": "2024-08-16T08:00:49.752526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.89473684 0.94736842 0.94736842 1.         0.94736842 0.94736842\n",
      " 0.94736842 0.94736842 0.94736842 0.94444444]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the Feature Importances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Print feature importances of the selected features\n",
    "feature_importances = clf.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "feature_importances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T08:00:50.839366Z",
     "start_time": "2024-08-16T08:00:50.828798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  feature  importance\n",
       "6    satellite_cn0_median    0.169190\n",
       "4       satellite_cn0_max    0.159368\n",
       "2       nrSatellitesInFix    0.130355\n",
       "7      satellite_cn0_mode    0.098567\n",
       "5      satellite_cn0_mean    0.098037\n",
       "9     satellite_cn0_range    0.094234\n",
       "8       satellite_cn0_std    0.071768\n",
       "3       satellite_cn0_min    0.041520\n",
       "13  bluetooth_rssi_median    0.016981\n",
       "1      nrSatellitesInView    0.016389\n",
       "0            cellStrength    0.014837\n",
       "17          wifi_rssi_min    0.014615\n",
       "19         wifi_rssi_mean    0.014577\n",
       "20       wifi_rssi_median    0.012292\n",
       "12    bluetooth_rssi_mean    0.011916\n",
       "18          wifi_rssi_max    0.006562\n",
       "21         wifi_rssi_mode    0.004595\n",
       "14    bluetooth_rssi_mode    0.004442\n",
       "10     bluetooth_rssi_min    0.004192\n",
       "22        wifi_rssi_range    0.003996\n",
       "16   bluetooth_rssi_range    0.003958\n",
       "11     bluetooth_rssi_max    0.003827\n",
       "15     bluetooth_rssi_std    0.003781"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>satellite_cn0_median</td>\n",
       "      <td>0.169190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>satellite_cn0_max</td>\n",
       "      <td>0.159368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nrSatellitesInFix</td>\n",
       "      <td>0.130355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>satellite_cn0_mode</td>\n",
       "      <td>0.098567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>satellite_cn0_mean</td>\n",
       "      <td>0.098037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>satellite_cn0_range</td>\n",
       "      <td>0.094234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>satellite_cn0_std</td>\n",
       "      <td>0.071768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>satellite_cn0_min</td>\n",
       "      <td>0.041520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bluetooth_rssi_median</td>\n",
       "      <td>0.016981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nrSatellitesInView</td>\n",
       "      <td>0.016389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cellStrength</td>\n",
       "      <td>0.014837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wifi_rssi_min</td>\n",
       "      <td>0.014615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wifi_rssi_mean</td>\n",
       "      <td>0.014577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wifi_rssi_median</td>\n",
       "      <td>0.012292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bluetooth_rssi_mean</td>\n",
       "      <td>0.011916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>wifi_rssi_max</td>\n",
       "      <td>0.006562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>wifi_rssi_mode</td>\n",
       "      <td>0.004595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bluetooth_rssi_mode</td>\n",
       "      <td>0.004442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bluetooth_rssi_min</td>\n",
       "      <td>0.004192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wifi_rssi_range</td>\n",
       "      <td>0.003996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bluetooth_rssi_range</td>\n",
       "      <td>0.003958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bluetooth_rssi_max</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bluetooth_rssi_std</td>\n",
       "      <td>0.003781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model and tryout the model with new data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model_path = '../Daten/random_forest_classifier.joblib'\n",
    "joblib.dump(clf, model_path)\n",
    "print(f'Saved model to {model_path}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T08:00:50.862449Z",
     "start_time": "2024-08-16T08:00:50.840323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../Daten/random_forest_classifier.joblib\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "# Test the rf model with new data in validation folder in this notebook",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "validation_files_directory = '../Daten/validation/'\n",
    "validation_averaged_path = '../Daten/validation_averaged_data.csv'\n",
    "model_path = '../Daten/random_forest_classifier.joblib'\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "validation_averaged_data = pd.DataFrame()\n",
    "validation_description = pd.DataFrame()\n",
    "validation_files = [file for file in os.listdir(validation_files_directory) if file.endswith('.csv')]\n",
    "\n",
    "for file in validation_files:\n",
    "    df = pd.read_csv(validation_files_directory + file, sep=';')\n",
    "    locationDescription = df['locationDescription'][0]\n",
    "    locationPeople = df['people'][0]\n",
    "    \n",
    "    validation_description = pd.concat([validation_description, pd.DataFrame({'locationDescription': [locationDescription], 'people': [locationPeople], 'file': [file]})], ignore_index=True)\n",
    "\n",
    "    validation_averaged_data = preprocess_data(validation_averaged_data, validation_files_directory, file)\n",
    "\n",
    "# Predict the labels of the validation data\n",
    "validation_predictions = model.predict(validation_averaged_data.drop(columns=['label']))\n",
    "validation_probabilities = model.predict_proba(validation_averaged_data.drop(columns=['label']))\n",
    "validation_probabilities = [max(probability) for probability in validation_probabilities]\n",
    "\n",
    "#Save all predictions an information in a dataframe\n",
    "validation_predictions_rfc = pd.DataFrame()\n",
    "\n",
    "#print label and the corresponding prediction\n",
    "for label, description, people, prediction, probability, file in zip(validation_averaged_data['label'], validation_description['locationDescription'], validation_description['people'], validation_predictions, validation_probabilities, validation_description['file']):\n",
    "    validation_predictions_rfc = pd.concat([validation_predictions_rfc, pd.DataFrame({'label': [label], 'prediction': [prediction], 'probability': [probability],'description': [description], 'people': [people], 'file': [file]})], ignore_index=True)\n",
    "    '''print(\"--------------------\")\n",
    "    print(f'Label: {label}, Prediction: {prediction}, Description: {description}, People: {people},  Probability: {probability}, File: {file}')'''\n",
    "   \n",
    "data = validation_predictions_rfc\n",
    "\n",
    "# Calculate the average confidence  and accuracy for each location description and people\n",
    "\n",
    "for condition in data['description'].unique():\n",
    "    subset = data[data['description'] == condition]\n",
    "    mean_confidence = subset['probability'].mean()\n",
    "    print(f\"Average confidence for {condition}: {mean_confidence:.2f}\")\n",
    "    \n",
    "for condition in data['description'].unique():\n",
    "    condition_mask = data['description'] == condition\n",
    "    accuracy = accuracy_score(data[condition_mask]['label'], data[condition_mask]['prediction'])\n",
    "    print(f\"Accuracy for {condition}: {accuracy}\")\n",
    "    \n",
    "\n",
    "print(\"__--------_______-----___\")\n",
    "\n",
    "\n",
    "for condition in data['people'].unique():\n",
    "    subset = data[data['people'] == condition]\n",
    "    mean_confidence = subset['probability'].mean()\n",
    "    print(f\"Average confidence for {condition}: {mean_confidence:.2f}\")\n",
    "    \n",
    "for condition in data['people'].unique():\n",
    "    condition_mask = data['people'] == condition\n",
    "    accuracy = accuracy_score(data[condition_mask]['label'], data[condition_mask]['prediction'])\n",
    "    print(f\"Accuracy for {condition}: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T08:00:52.560945Z",
     "start_time": "2024-08-16T08:00:50.863678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confidence for Überdacht: 0.89\n",
      "Average confidence for Kein Fenster: 1.00\n",
      "Average confidence for Nähe Fenster: 0.86\n",
      "Average confidence for Häuserschlucht: 0.94\n",
      "Average confidence for Raummitte: 1.00\n",
      "Average confidence for Frei: 0.96\n",
      "Accuracy for Überdacht: 1.0\n",
      "Accuracy for Kein Fenster: 1.0\n",
      "Accuracy for Nähe Fenster: 0.7272727272727273\n",
      "Accuracy for Häuserschlucht: 1.0\n",
      "Accuracy for Raummitte: 1.0\n",
      "Accuracy for Frei: 1.0\n",
      "__--------_______-----___\n",
      "Average confidence for viel: 0.89\n",
      "Average confidence for keine: 0.93\n",
      "Average confidence for weniger als 5: 0.95\n",
      "Accuracy for viel: 0.8\n",
      "Accuracy for keine: 0.9333333333333333\n",
      "Accuracy for weniger als 5: 1.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T08:02:46.834809Z",
     "start_time": "2024-08-16T08:01:41.596540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.saving.save import load_model\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "dropout_rate = 0.1\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a classifier with the wrapper\n",
    "model = KerasClassifier(model=create_model, epochs=15, batch_size=32, verbose=1)\n",
    "\n",
    "#convert all columns to float except the label column\n",
    "'''X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.)\n",
    "X_train = X_train.astype(float)\n",
    "Y_train = Y_train.map({'Indoor': 0, 'Outdoor': 1})\n",
    "Y_train = Y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "Y_test = Y_test.map({'Indoor': 0, 'Outdoor': 1})\n",
    "Y_train = Y_train.astype(float)'''\n",
    "\n",
    "# Don't split the data into training and testing data but use all for training, since we have a separate validation dataset\n",
    "X_train = X\n",
    "Y_train = Y.map({'Indoor': 0, 'Outdoor': 1})\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "Y_train = Y_train.astype(float)\n",
    "#X_test = np.array(X_test)\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "\n",
    "#Uncomment the following line to train the model and comment line 56 and 57\n",
    "model.fit(X_train, Y_train, batch_size=32)\n",
    "\n",
    "# Load the final tflite model instead of retraining it\n",
    "#model_path = '../Daten/lstm_classifier.h5'\n",
    "#model = load_model(model_path)\n",
    "\n",
    "# Now you can use cross_val_score (this works only with \n",
    "scores = cross_val_score(model, X_train, Y_train, cv=10)\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "\n",
    "# Save the model\n",
    "model_path = '../Daten/lstm_classifier.h5'\n",
    "if hasattr(model, 'model_'):  # Check if the fitted model is accessible\n",
    "    #model.model_.save(model_path)  # Note the underscore in model_\n",
    "    print(f'Saved model to {model_path}')\n",
    "else:\n",
    "    print(\"Model has not been fitted or model is not< accessible\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (189, 23, 1)\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 10:01:41.972538: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 39ms/step - loss: 0.6845 - accuracy: 0.5556\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.5474 - accuracy: 0.7302\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.3586 - accuracy: 0.8571\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2428 - accuracy: 0.8942\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.2165 - accuracy: 0.9153\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.2577 - accuracy: 0.8783\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1759 - accuracy: 0.9365\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1819 - accuracy: 0.9365\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.2197 - accuracy: 0.8889\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1839 - accuracy: 0.9206\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1627 - accuracy: 0.9418\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1818 - accuracy: 0.9153\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1729 - accuracy: 0.9259\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1565 - accuracy: 0.9471\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1332 - accuracy: 0.9418\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 34ms/step - loss: 0.6588 - accuracy: 0.5941\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.5625 - accuracy: 0.6647\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.4810 - accuracy: 0.7706\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.3168 - accuracy: 0.8882\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.4953 - accuracy: 0.7765\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.4344 - accuracy: 0.7765\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.4507 - accuracy: 0.7235\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.4041 - accuracy: 0.7941\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.3246 - accuracy: 0.8529\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1853 - accuracy: 0.9412\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1947 - accuracy: 0.9471\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1567 - accuracy: 0.9353\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1986 - accuracy: 0.9176\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1886 - accuracy: 0.9118\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1790 - accuracy: 0.9294\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 37ms/step - loss: 0.6850 - accuracy: 0.5882\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.5531 - accuracy: 0.7471\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.4018 - accuracy: 0.8176\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2867 - accuracy: 0.8882\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2280 - accuracy: 0.9118\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.3551 - accuracy: 0.8588\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2688 - accuracy: 0.8765\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.2594 - accuracy: 0.8941\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2911 - accuracy: 0.8647\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2315 - accuracy: 0.9000\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2016 - accuracy: 0.9176\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.3003 - accuracy: 0.8824\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2060 - accuracy: 0.9000\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.2627 - accuracy: 0.8706\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1833 - accuracy: 0.9235\n",
      "1/1 [==============================] - 1s 647ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 32ms/step - loss: 0.6507 - accuracy: 0.6000\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.5634 - accuracy: 0.6882\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.4289 - accuracy: 0.8118\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.3152 - accuracy: 0.9059\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.3474 - accuracy: 0.8529\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2686 - accuracy: 0.8824\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.2087 - accuracy: 0.9235\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.2007 - accuracy: 0.9176\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.2218 - accuracy: 0.9118\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1980 - accuracy: 0.9118\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.2570 - accuracy: 0.8941\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.2529 - accuracy: 0.8765\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2028 - accuracy: 0.9353\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1529 - accuracy: 0.9235\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1421 - accuracy: 0.9353\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 36ms/step - loss: 0.6439 - accuracy: 0.6471\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.4309 - accuracy: 0.8176\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.5749 - accuracy: 0.7235\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.4814 - accuracy: 0.7765\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.3969 - accuracy: 0.8235\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2745 - accuracy: 0.9235\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.3024 - accuracy: 0.8706\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2938 - accuracy: 0.8647\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.2028 - accuracy: 0.9353\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1848 - accuracy: 0.9294\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1539 - accuracy: 0.9353\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1908 - accuracy: 0.9294\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1446 - accuracy: 0.9294\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1407 - accuracy: 0.9294\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1520 - accuracy: 0.9353\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 34ms/step - loss: 0.6637 - accuracy: 0.5647\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.4995 - accuracy: 0.7588\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.4751 - accuracy: 0.7706\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.3615 - accuracy: 0.8353\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.4015 - accuracy: 0.7824\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.3817 - accuracy: 0.8706\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2410 - accuracy: 0.9176\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2165 - accuracy: 0.9176\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2454 - accuracy: 0.9294\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1593 - accuracy: 0.9353\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1786 - accuracy: 0.9294\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1361 - accuracy: 0.9412\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1574 - accuracy: 0.9471\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2181 - accuracy: 0.9235\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.3786 - accuracy: 0.8176\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x28028ef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 399ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 33ms/step - loss: 0.6518 - accuracy: 0.6000\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.4207 - accuracy: 0.8000\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.3442 - accuracy: 0.8235\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2337 - accuracy: 0.9059\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1545 - accuracy: 0.9353\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1630 - accuracy: 0.9353\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1585 - accuracy: 0.9176\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.1745 - accuracy: 0.9176\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.1548 - accuracy: 0.9294\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1336 - accuracy: 0.9294\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1290 - accuracy: 0.9353\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1214 - accuracy: 0.9471\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1300 - accuracy: 0.9471\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1419 - accuracy: 0.9412\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1988 - accuracy: 0.9294\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2874eb5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 811ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 33ms/step - loss: 0.6311 - accuracy: 0.6000\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.5605 - accuracy: 0.6529\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.3999 - accuracy: 0.8235\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2430 - accuracy: 0.9412\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7163 - accuracy: 0.8059\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.5441 - accuracy: 0.7882\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.4417 - accuracy: 0.7824\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.4378 - accuracy: 0.7824\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.4235 - accuracy: 0.7824\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.4216 - accuracy: 0.7824\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.3535 - accuracy: 0.8353\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2595 - accuracy: 0.9294\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1802 - accuracy: 0.9294\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.2017 - accuracy: 0.9176\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.2919 - accuracy: 0.8765\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 33ms/step - loss: 0.6772 - accuracy: 0.5235\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.4685 - accuracy: 0.7941\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.3246 - accuracy: 0.8706\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.2468 - accuracy: 0.9000\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.2564 - accuracy: 0.8941\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.3133 - accuracy: 0.8529\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2163 - accuracy: 0.9118\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2381 - accuracy: 0.9176\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2131 - accuracy: 0.9118\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1487 - accuracy: 0.9412\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1496 - accuracy: 0.9471\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1406 - accuracy: 0.9529\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1279 - accuracy: 0.9471\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1488 - accuracy: 0.9353\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1439 - accuracy: 0.9529\n",
      "1/1 [==============================] - 1s 879ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 32ms/step - loss: 0.6741 - accuracy: 0.5706\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5225 - accuracy: 0.7412\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.3380 - accuracy: 0.8647\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2330 - accuracy: 0.9176\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.2691 - accuracy: 0.9235\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1615 - accuracy: 0.9353\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1900 - accuracy: 0.9294\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1930 - accuracy: 0.9353\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1641 - accuracy: 0.9588\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.2130 - accuracy: 0.9000\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1477 - accuracy: 0.9529\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1513 - accuracy: 0.9647\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1717 - accuracy: 0.9471\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1610 - accuracy: 0.9353\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.1657 - accuracy: 0.9471\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "Epoch 1/15\n",
      "6/6 [==============================] - 2s 36ms/step - loss: 0.6486 - accuracy: 0.6023\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.4959 - accuracy: 0.7310\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.3211 - accuracy: 0.8596\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.2120 - accuracy: 0.9123\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.2594 - accuracy: 0.9006\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.3022 - accuracy: 0.9181\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.2678 - accuracy: 0.8830\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.2802 - accuracy: 0.8947\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2144 - accuracy: 0.9181\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1864 - accuracy: 0.9298\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.1761 - accuracy: 0.9415\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1624 - accuracy: 0.9240\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1972 - accuracy: 0.9357\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1824 - accuracy: 0.9415\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1832 - accuracy: 0.9298\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "Cross-validation scores: [0.94736842 0.94736842 1.         1.         0.84210526 0.84210526\n",
      " 0.94736842 0.89473684 0.94736842 0.77777778]\n",
      "Saved model to ../Daten/lstm_classifier.h5\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "\n",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert the LSTM Model to TFLite"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Initialize the TFLite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Use the Select TF ops and disable the _experimental_lower_tensor_list_ops flag\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open(\"../Daten/lstm_classifier.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f'Saved model to ../Daten/lstm_classifier.tflite')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Validate the LSTM Model with h5 file",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "import os\n",
    "import pandas as pd\n",
    "import json \n",
    "from collections import Counter\n",
    "\n",
    "# Load the validation data into dataframes\n",
    "validation_averaged_data = pd.DataFrame()\n",
    "validation_description = pd.DataFrame()\n",
    "validation_files = [file for file in os.listdir(validation_files_directory) if file.endswith('.csv')]\n",
    "\n",
    "for file in validation_files:\n",
    "    df = pd.read_csv(validation_files_directory + file, sep=';')\n",
    "    locationDescription = df['locationDescription'][0]\n",
    "    locationPeople = df['people'][0]\n",
    "    \n",
    "    validation_description = pd.concat([validation_description, pd.DataFrame({'locationDescription': [locationDescription], 'people': [locationPeople], 'file': [file]})], ignore_index=True)\n",
    "\n",
    "    validation_averaged_data = preprocess_data(validation_averaged_data, validation_files_directory, file)\n",
    "\n",
    "#map the label column to 0 and 1\n",
    "validation_averaged_data['label'] = validation_averaged_data['label'].map({'Indoor': 0, 'Outdoor': 1})\n",
    "\n",
    "#convert all columns to float except the label column\n",
    "validation_averaged_data = validation_averaged_data.astype(float)\n",
    "\n",
    "Y_validation = validation_averaged_data['label']\n",
    "X_validation = validation_averaged_data.drop(columns=['label'])\n",
    "\n",
    "X_validation = np.array(X_validation)\n",
    "X_validation = np.reshape(X_validation, (X_validation.shape[0], X_validation.shape[1], 1))\n",
    "\n",
    "Y_validation = Y_validation.astype(float)\n",
    "\n",
    "#load the model\n",
    "model_path = '../Daten/lstm_classifier.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "#predict the labels of the validation data for each data point with their corresponding probability\n",
    "validation_predictions = model.predict(X_validation)\n",
    "validation_predictions = [np.argmax(prediction) for prediction in validation_predictions]\n",
    "validation_probabilities = model.predict_proba(X_validation)\n",
    "validation_probabilities = [max(probability) for probability in validation_probabilities]\n",
    "\n",
    "#convert label and prediction to Indoor and Outdoor\n",
    "validation_averaged_data['label'] = validation_averaged_data['label'].map({0: 'Indoor', 1: 'Outdoor'})\n",
    "validation_predictions = ['Indoor' if prediction == 0 else 'Outdoor' for prediction in validation_predictions]\n",
    "\n",
    "#Save all predictions an information in a dataframe\n",
    "validation_predictions_lstm = pd.DataFrame()\n",
    "\n",
    "for label, description, people, prediction, probability, file in zip(validation_averaged_data['label'], validation_description['locationDescription'], validation_description['people'], validation_predictions, validation_probabilities, validation_description['file']):\n",
    "    validation_predictions_lstm = pd.concat([validation_predictions_lstm, pd.DataFrame({'label': [label], 'prediction': [prediction], 'probability': [probability],'description': [description], 'people': [people], 'file': [file]})], ignore_index=True)\n",
    "'''#print label and the corresponding prediction\n",
    "for label, description, people, prediction, file in zip(validation_averaged_data['label'], validation_description['locationDescription'], validation_description['people'], validation_predictions, validation_description['file']):\n",
    "    #print(f'Label: {label}, Prediction: {prediction}, Description: {description}, People: {people},  File: {file}')'''\n",
    "\n",
    "data = validation_predictions_lstm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for condition in data['description'].unique():\n",
    "    subset = data[data['description'] == condition]\n",
    "    mean_confidence = subset['probability'].mean()\n",
    "    print(f\"Average confidence for {condition}: {mean_confidence:.2f}\")\n",
    "    \n",
    "for condition in data['description'].unique():\n",
    "    condition_mask = data['description'] == condition\n",
    "    accuracy = accuracy_score(data[condition_mask]['label'], data[condition_mask]['prediction'])\n",
    "    print(f\"Accuracy for {condition}: {accuracy}\")\n",
    "    \n",
    "\n",
    "print(\"__--------_______-----___\")\n",
    "\n",
    "\n",
    "for condition in data['people'].unique():\n",
    "    subset = data[data['people'] == condition]\n",
    "    mean_confidence = subset['probability'].mean()\n",
    "    print(f\"Average confidence for {condition}: {mean_confidence:.2f}\")\n",
    "    \n",
    "for condition in data['people'].unique():\n",
    "    condition_mask = data['people'] == condition\n",
    "    accuracy = accuracy_score(data[condition_mask]['label'], data[condition_mask]['prediction'])\n",
    "    print(f\"Accuracy for {condition}: {accuracy}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validate the LSTM Model with tflite file"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''# run the same validation process as above but with the tflite model\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"../Daten/lstm_classifier.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load the validation data into dataframes\n",
    "validation_averaged_data = pd.DataFrame()\n",
    "validation_description = pd.DataFrame()\n",
    "validation_files = [file for file in os.listdir(validation_files_directory) if file.endswith('.csv')]\n",
    "for file in validation_files:\n",
    "    df = pd.read_csv(validation_files_directory + file, sep=';')\n",
    "    locationDescription = df['locationDescription'][0]\n",
    "    locationPeople = df['people'][0]\n",
    "    \n",
    "    validation_description = pd.concat([validation_description, pd.DataFrame({'locationDescription': [locationDescription], 'people': [locationPeople], 'file': [file]})], ignore_index=True)\n",
    "\n",
    "    validation_averaged_data = preprocess_data(validation_averaged_data, validation_files_directory, file)\n",
    "    \n",
    "#map the label column to 0 and 1\n",
    "validation_averaged_data['label'] = validation_averaged_data['label'].map({'Indoor': 0, 'Outdoor': 1})\n",
    "\n",
    "#convert all columns to float except the label column\n",
    "validation_averaged_data = validation_averaged_data.astype(float)\n",
    "\n",
    "Y_validation = validation_averaged_data['label']\n",
    "X_validation = validation_averaged_data.drop(columns=['label'])\n",
    "\n",
    "X_validation = np.array(X_validation)\n",
    "X_validation = np.reshape(X_validation, (X_validation.shape[0], X_validation.shape[1], 1))\n",
    "\n",
    "Y_validation = Y_validation.astype(float)\n",
    "\n",
    "# Run the model with TensorFlow Lite\n",
    "validation_predictions = []\n",
    "for i in range(len(X_validation)):\n",
    "    # Add an extra dimension to the input tensor\n",
    "    input_data = np.expand_dims(X_validation[i], axis=0).astype(np.float32)\n",
    "\n",
    "    # Set the tensor to point to the input data to be inferred\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve the output of the inference\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    validation_predictions.append(output_data)\n",
    "    \n",
    "#print label and the corresponding prediction\n",
    "for label, description, people, prediction, file in zip(validation_averaged_data['label'], validation_description['locationDescription'], validation_description['people'], validation_predictions, validation_description['file']):\n",
    "    #if abs(prediction[0] - prediction[1]) < 0.7:\n",
    "    print(f'Label: {label}, Prediction: {prediction}, Description: {description}, People: {people},  File: {file}')'''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confusion Matrix and Probability Distribution"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define true labels and predictions for RFC with the updated data\n",
    "true_labels_rfc = [\n",
    "    'Outdoor', 'Indoor', 'Indoor', 'Outdoor', 'Indoor', 'Indoor', 'Indoor', 'Outdoor', 'Outdoor', 'Outdoor', 'Outdoor', 'Indoor', 'Indoor', 'Outdoor', 'Outdoor', 'Indoor', 'Indoor', 'Indoor', 'Outdoor', 'Indoor', 'Outdoor', 'Outdoor', 'Indoor', 'Indoor', 'Outdoor', 'Indoor', 'Indoor', 'Outdoor', 'Indoor', 'Indoor', 'Indoor'\n",
    "]\n",
    "predictions_rfc_alphabetical = [\n",
    "    'Outdoor','Indoor','Indoor','Outdoor','Indoor','Indoor','Indoor','Outdoor','Outdoor','Outdoor','Outdoor','Outdoor','Indoor','Outdoor','Outdoor','Outdoor','Indoor','Indoor','Outdoor','Outdoor','Outdoor','Outdoor','Indoor','Indoor','Outdoor','Indoor','Indoor','Outdoor','Indoor','Indoor','Indoor'\n",
    "]\n",
    "\n",
    "predictions_rfc_probs = [\n",
    "    [0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [0.18, 0.82], [1.0, 0.0], [0.85, 0.15], [1.0, 0.0], [0.13, 0.87], [0.0, 1.0], [0.02, 0.98], [0.0, 1.0], [0.41, 0.59], [1.0, 0.0], [0.06, 0.94], [0.0, 1.0], [0.23, 0.77], [1.0, 0.0],[1.0, 0.0], [0.07, 0.93], [0.28, 0.72], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.96, 0.04], [0.21, 0.79], [0.99, 0.01], [0.87, 0.13], [0.29, 0.71], [0.74, 0.26], [1.0, 0.0], [1.0,0.0]\n",
    "]\n",
    "\n",
    "print(\"Predictions rfc: \", predictions_rfc_probs)\n",
    "\n",
    "# Define true labels and predictions for LSTM with the updated data\n",
    "true_labels_lstm = [\n",
    "     1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "]\n",
    "predictions_lstm_probs = [\n",
    "    [0.03169885, 0.9683012], [9.9961096e-01, 3.8903771e-04], [9.9962103e-01, 3.7897032e-04], [0.0628316, 0.9371685], [9.9950373e-01, 4.9623835e-04], [0.96669286, 0.03330712], [9.9960977e-01, 3.9024084e-04], [0.03111412, 0.96888584], [0.02428848, 0.9757116], [0.02169448, 0.9783055] ,[0.02275957, 0.9772404], [0.538785, 0.46121508], [9.9961084e-01, 3.8915250e-04], [0.07990491, 0.9200951], [0.02168687, 0.9783132], [0.5124758, 0.48752418], [9.9961191e-01, 3.8806704e-04], [9.9961460e-01, 3.8542488e-04], [0.07851861, 0.9214814], [0.17278534, 0.8272147 ], [0.02279828, 0.9772017], [0.022396, 0.97760403], [9.9960881e-01, 3.9122254e-04], [0.96038276, 0.03961726], [0.05924854, 0.94075143], [9.9958938e-01, 4.1067746e-04], [0.9418696, 0.05813045], [0.10422354, 0.89577645], [0.7670185, 0.23298149], [9.9961120e-01, 3.8879746e-04], [0.992029, 0.007971]\n",
    "]\n",
    "print(\"Predictions lstm: \", predictions_lstm_probs)\n",
    "predictions_lstm = [np.argmax(pred) for pred in predictions_lstm_probs]\n",
    "\n",
    "# Ensure true_labels_lstm and predictions_lstm have the same length\n",
    "assert len(true_labels_lstm) == len(predictions_lstm), \"Length mismatch between true labels and predictions\"\n",
    "\n",
    "# Compute confusion matrices\n",
    "conf_matrix_rfc = confusion_matrix(true_labels_rfc, predictions_rfc_alphabetical, labels=['Indoor', 'Outdoor'])\n",
    "conf_matrix_lstm = confusion_matrix(true_labels_lstm, predictions_lstm, labels=[0.0, 1.0])\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fontsize = 12\n",
    "\n",
    "# Plot RFC confusion matrix\n",
    "sns.heatmap(conf_matrix_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=['Indoor', 'Outdoor'], yticklabels=['Indoor', 'Outdoor'], ax=ax[0], annot_kws={\"size\": fontsize})\n",
    "ax[0].set_title('RFC Confusion Matrix', fontsize=fontsize)\n",
    "ax[0].set_xlabel('Predicted', fontsize=fontsize)\n",
    "ax[0].set_ylabel('Actual', fontsize=fontsize)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "# Plot LSTM confusion matrix\n",
    "sns.heatmap(conf_matrix_lstm, annot=True, fmt='d', cmap='Blues', xticklabels=['Indoor', 'Outdoor'], yticklabels=['Indoor', 'Outdoor'], ax=ax[1], annot_kws={\"size\": fontsize})\n",
    "ax[1].set_title('LSTM Confusion Matrix', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Predicted', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Actual', fontsize=fontsize)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrices.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot probability distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot RFC probabilities\n",
    "sns.histplot([prob[1] for prob in predictions_rfc_probs], bins=10, kde=False, ax=ax[0],legend=False)\n",
    "ax[0].set_title('RFC Probability Distribution', fontsize=fontsize)\n",
    "ax[0].set_xlabel('Probability of Outdoor', fontsize=fontsize)\n",
    "ax[0].set_ylabel('Frequency', fontsize=fontsize)\n",
    "ax[0].set_ylim(0, 15)  # Set y-axis limit to 14\n",
    "\n",
    "# Plot LSTM probabilities\n",
    "sns.histplot([prob[1] for prob in predictions_lstm_probs], bins=10, kde=False, ax=ax[1], legend=False)\n",
    "ax[1].set_title('LSTM Probability Distribution', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Probability of Outdoor', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Frequency', fontsize=fontsize)\n",
    "ax[1].set_ylim(0, 15)  # Set y-axis limit to 14\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"probability_distributions.png\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model Comparison"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'y_true' holds actual labels and 'y_pred_rfc', 'y_pred_lstm' hold predictions from both models\n",
    "accuracy_rfc = accuracy_score(true_labels_rfc, predictions_rfc)\n",
    "precision_rfc = precision_score(true_labels_lstm, predictions_lstm, average='macro')\n",
    "recall_rfc = recall_score(true_labels_rfc, predictions_rfc, average='macro')\n",
    "f1_rfc = f1_score(true_labels_rfc, predictions_rfc, average='macro')\n",
    "confidence_rfc = np.mean(predictions_rfc_probs)\n",
    "\n",
    "accuracy_lstm = accuracy_score(true_labels_lstm, predictions_lstm)\n",
    "precision_lstm = precision_score(true_labels_lstm, predictions_lstm, average='macro')\n",
    "recall_lstm = recall_score(true_labels_lstm, predictions_lstm, average='macro')\n",
    "f1_lstm = f1_score(true_labels_lstm, predictions_lstm, average='macro')\n",
    "confidence_lstm = np.mean([max(prob) for prob in predictions_lstm_probs])\n",
    "\n",
    "print(f\"RFC Metrics: Accuracy={accuracy_rfc}, Precision={precision_rfc}, Recall={recall_rfc}, F1-Score={f1_rfc}\")\n",
    "print(f\"Confidence: \", {confidence_rfc})\n",
    "print(f\"LSTM Metrics: Accuracy={accuracy_lstm}, Precision={precision_lstm}, Recall={recall_lstm}, F1-Score={f1_lstm}\")\n",
    "print(f\"Confidence: \", {confidence_lstm})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analyze transition scenarios"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "transition_scenarios_folder = '../Daten/environment_transition/'\n",
    "transition_files = ['LC_Indoor_to_Outdoor.csv', 'LC_Outdoor_to_Indoor.csv', 'BIB_Outdoor_to_Indoor.csv', 'Sparkasse_Outdoor_to_Indoor.csv']\n",
    "\n",
    "# Load the transition scenarios into dataframes\n",
    "Indoor_to_Outdoor_LC = pd.read_csv(transition_scenarios_folder + 'LC_Indoor_to_Outdoor.csv')\n",
    "Outdoor_to_Indoor_LC = pd.read_csv(transition_scenarios_folder + 'LC_Outdoor_to_Indoor.csv')\n",
    "Outdoor_to_Indoor_Bib = pd.read_csv(transition_scenarios_folder + 'BIB_Outdoor_to_Indoor.csv')\n",
    "Outdoor_to_Indoor_Sparkasse = pd.read_csv(transition_scenarios_folder + 'Sparkasse_Outdoor_to_Indoor.csv')\n",
    "\n",
    "data_frames = [Indoor_to_Outdoor_LC, Outdoor_to_Indoor_LC, Outdoor_to_Indoor_Bib, Outdoor_to_Indoor_Sparkasse]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "axs = axs.flatten()\n",
    "fontsize = 16\n",
    "# Loop over each scenario and plot\n",
    "for ax, data, scenario_name in zip(axs, data_frames, transition_files):\n",
    "    # Assume 'probability' column exists which has the probability values\n",
    "    # Change 'predictionRFC_Indoor' to 'predictionLSTM_Indoor' to see results of other model\n",
    "    ax.plot(data['predictionRFC_Indoor'], label=f'Indoor Probability of {scenario_name[:-4]}')  # Removes '.csv' from name\n",
    "    ax.set_title(f'Probability Over Time for {scenario_name[:-4]}', fontsize=fontsize)\n",
    "    ax.set_xlabel('Time (seconds)', fontsize=fontsize)\n",
    "    ax.set_ylabel('Probability', fontsize=fontsize)\n",
    "    ax.tick_params(axis='both', labelsize=fontsize)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check when the models are predicting the transition by looking at the probability values\n",
    "# Print the index of for all four scenarios when the model predicts the transition\n",
    "\n",
    "# Define the threshold for transition prediction\n",
    "threshold = 0.5\n",
    "transition_indices_LSTM = []\n",
    "transition_indices_RFC = []\n",
    "\n",
    "# Loop over each scenario and print the index when the transition is predicted by RFC\n",
    "print(\"Transition predicted by RFC\")\n",
    "for data, scenario_name in zip(data_frames, transition_files):\n",
    "    # Assume 'probability' column exists which has the probability values\n",
    "    transition_indices_RFC = data[data['predictionRFC_Indoor'] > threshold].index\n",
    "    print(f'Transition predicted for {scenario_name[:-4]} at indices: {transition_indices_RFC}')\n",
    "    \n",
    "print()\n",
    "\n",
    "# Loop over each scenario and print the index when the transition is predicted by LSTM\n",
    "print(\"Transition predicted by LSTM\")\n",
    "for data, scenario_name in zip(data_frames, transition_files):\n",
    "    # Assume 'probability' column exists which has the probability values\n",
    "    transition_indices_LSTM = data[data['predictionLSTM_Indoor'] > threshold].index\n",
    "    print(f'Transition predicted for {scenario_name[:-4]} at indices: {transition_indices_LSTM}')\n",
    "\n",
    "\n",
    "# For the sake of simplicity (since there are only 4 scenarios), we will calculate the delay manually\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
